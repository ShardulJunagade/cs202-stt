\documentclass[10pt,a4paper]{report}

% Packages
\usepackage[a4paper,margin=0.9in]{geometry} % reduce margins
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{graphicx} % for images
\usepackage{caption}  % better caption control
\usepackage{lipsum}   % for dummy text (remove later)
\usepackage{enumitem}
\usepackage{amssymb} % for \checkmark symbol
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[L]{CS202 Lab Assignment 1 Report}
\fancyhead[R]{Shardul Junagade}
\fancyfoot[C]{\thepage}
\usepackage{xcolor}
\newcommand{\command}[1]{\texttt{\textcolor{blue}{#1}}}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.6em}

% Remove figure numbering
\captionsetup[figure]{labelformat=empty}

% Customize section/chapter fonts
\titleformat{\chapter}[hang]{\huge\bfseries}{}{0.4em}{} 
\titleformat{\section}{\Large\bfseries}{\thesection}{0.4em}{} 
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{0.4em}{} 

% Define custom command for images with optional caption
\usepackage{xparse}
\NewDocumentCommand{\insertimage}{O{0.8\textwidth} m o}{%
    \begin{center}
        \includegraphics[width=#1]{#2}%
        \IfValueT{#3}{\\\captionof{figure}{#3}}%
    \end{center}
}

% Front Page Info
\title{\Huge Assignment-1 \\[0.5cm] \LARGE Software Tools and Techniques for CSE}
\author{\Large Shardul Junagade (23110297) \\[0.2cm] \Large Repository: \href{https://github.com/ShardulJunagade/cs202-stt}{cs202-stt}}
\date{\large \today}

\begin{document}

% Front Page
\maketitle
\newpage

% Table of Contents
\tableofcontents
\newpage


% ---------- Lab 1 ----------
\chapter{Lab 1: Introduction to Version Controlling, Git Workflows, and Actions}


\section{Introduction, Setup, and Tools}

\subsection{Introduction}
The objective of this lab was to gain hands-on experience with Version Control Systems (VCS), specifically Git, and to explore the integration of GitHub with automated workflows such as GitHub Actions. My aim was to understand fundamental concepts like repositories, commits, branches, and remotes, and to perform essential Git operations. Additionally, I set up a continuous integration workflow using GitHub Actions and Pylint to ensure code quality.

\subsection{Setup and Tools}
I logged in to my GitHub account in the web browser -- \href{https://github.com/ShardulJunagade}{ShardulJunagade} -- for version control. I installed Git and Visual Studio Code by downloading the installers from their official websites and following the installation instructions. After installation, I verified the installation by checking the versions of Git and Visual Studio Code.

\begingroup
The following is a list of the tools and technologies used in this lab:
\begin{itemize}[itemsep=0.05em, topsep=0pt]
    \item Operating System: Windows 11
    \item Terminal: PowerShell 7
    \item Git version: 2.42.0
    \item Code Editor: Visual Studio Code
    \item Python version: 3.13.7
    \item GitHub for remote repository hosting
    \item GitHub Actions for CI/CD pipelines
\end{itemize}
\endgroup

\insertimage[0.8\textwidth]{../images/lab1/1.png}[Figure 1: Verifying Git installation and version.]

\newpage
\section{Methodology and Execution}

\subsection{Initializing a Local Repository}
I created a new folder for the lab, named \texttt{Any\_Name} using the command \command{mkdir Any\_Name} and initialized the folder as a git repository using the command \command{git init}.
\insertimage[0.8\textwidth]{../images/lab1/2.png}[Figure 2: Initializing a new Git repository.]


\subsection{Local Git Configuration}
Since my global Git configuration was already set up from previous work, I decided to configure my name and email specifically for this repository using the local configuration commands \command{git config user.name} and \command{git config user.email}. This ensures that all commits in this repository are attributed with the correct identity, regardless of the global settings.
\insertimage[0.8\textwidth]{../images/lab1/3.png}[Figure 3: Configuring local Git user details.]

\subsection{Adding and Committing Files}
I created a new file named \texttt{README.md} and used the command \command{git status} to inspect the repository. The output indicated one untracked file. I then staged the file with \command{git add README.md} and confirmed the change with another \command{git status}, which now showed the file as staged.
\insertimage[0.8\textwidth]{../images/lab1/4.png}[Figure 4: Staging the README.md file for commit.]

Next, I committed the staged file using the command \command{git commit -m "Initial commit: Added README.md"}. To verify, I executed \command{git log}, which displayed the commit details along with the message and metadata.
\insertimage[0.8\textwidth]{../images/lab1/5.png}[Figure 5: Committing the file and viewing the commit history.]


\subsection{Working with Remote Repository}
I created a remote repository on GitHub named \texttt{Any\_Name}.

Repository Link: \url{https://github.com/ShardulJunagade/Any_Name}
\insertimage[0.8\textwidth]{../images/lab1/6.png}[Figure 6: Creating a new repository on GitHub.]

After that, I linked the remote repository to my local repository using the command \command{git remote add origin <URL>}. Once the connection was established, I pushed my local commits to GitHub with \command{git push -u origin main}.
\insertimage[0.8\textwidth]{../images/lab1/7.png}[Figure 7: Adding the remote and pushing local commits to GitHub.]

To confirm that everything worked correctly, I opened the repository on GitHub in my browser and verified that all the changes were successfully reflected online. We can see that the README.md file is present.
\insertimage[0.8\textwidth]{../images/lab1/8.png}[Figure 8: Verifying updates on GitHub.]

For the next step, I cloned the repository using the command \command{git clone <URL>}, and we can see that the README.md file is present in the freshly cloned repo.
\insertimage[0.8\textwidth]{../images/lab1/9.png}[Figure 9: Cloning a repository from GitHub.]

Then, I moved to GitHub in my web browser and added a new file named \texttt{main.py} that contains a simple Python program. I committed the changes directly on GitHub to the remote repository. This file was now visible in the remote repository on GitHub but not in my local repository.
\insertimage[0.8\textwidth]{../images/lab1/10.png}[Figure 10: Adding a greeting message in \texttt{main.py} via GitHub.]

Now, I had to pull these changes/commits from the remote repository to my local repository. To do this I executed the command \command{git pull origin main} to fetch and merge the changes. This pulled the new file into my local repository, keeping both versions in sync.
\insertimage[0.8\textwidth]{../images/lab1/11.png}[Figure 11: Pulling the latest changes from GitHub.]



\subsection{GitHub Actions -- Pylint Workflow}
The next task was to automate code quality checks using Pylint.

For this task, I wrote a Python file called \texttt{app.py} that contained a simple calculator program, which you can see below:
\insertimage[0.68\textwidth]{../images/lab1/12.png}[Figure 12: Initial version of \texttt{app.py} (calculator code).]

After saving the file, I staged, committed, and pushed it to GitHub.
\insertimage[0.8\textwidth]{../images/lab1/13.png}[Figure 13: Pushing the calculator code to GitHub.]

On GitHub, there was a section of suggested workflows for GitHub Actions. Here, I selected the Pylint workflow and clicked on \texttt{Configure}.
\insertimage[0.5\textwidth]{../images/lab1/14.png}[Figure 14: Suggested workflows for GitHub Actions.]

Then, I created the \texttt{.github/workflows/Pylint.yml} file to enable linting on every push, as shown below:
\insertimage[0.8\textwidth]{../images/lab1/15.png}[Figure 15: Creating the \texttt{Pylint.yml} workflow file.]

As soon as I committed the workflow file, GitHub Actions automatically triggered the workflow. The first run failed because Pylint reported several errors in my code, which can be seen below:
\insertimage[0.8\textwidth]{../images/lab1/16.png}[Figure 16: GitHub Actions running Pylint workflow.]
\insertimage[0.8\textwidth]{../images/lab1/17.png}[Figure 17: Initial Pylint errors detected.]

After reviewing the error messages, I fixed the issues by adding proper docstrings and correcting other bugs. I committed and pushed the corrected code to the repository.
% \insertimage[0.8\textwidth]{../images/lab1/18.png}[Figure 18: Updating the calculator code to address Pylint errors.]
\insertimage[0.8\textwidth]{../images/lab1/20.png}[Figure 19: Pushing the corrected code.]

\newpage
This time, the workflow passed successfully, and GitHub showed a green tick, which confirmed that my code met the required standards.
\insertimage[0.8\textwidth]{../images/lab1/21.png}[Figure 20: Successful Pylint workflow run.]

All three workflow runs completed successfully, as indicated by the green tick on each build in the GitHub Actions tab.
\insertimage[0.8\textwidth]{../images/lab1/22.png}[Figure 21: Workflow completion with a green tick.]



\section{Results and Analysis}

Through this lab, I successfully created and managed a Git repository, synchronized changes with GitHub, and demonstrated cloning and pulling operations. The GitHub Actions workflow validated my Python code using Pylint, ensuring it followed coding standards and confirming the results with a green tick on GitHub.

\insertimage[0.8\textwidth]{../images/lab1/23.png}[Figure 22: Successful Pylint validation and workflow status.]

The final commit history below shows the whole process -- from the first commit to fixing errors and reaching the successful workflow run.
\insertimage[0.8\textwidth]{../images/lab1/24.png}[Figure 23: Final commit history showing all changes.]


\begin{center}
\begin{tabular}{|p{4cm}|p{9cm}|}
\hline
\textbf{Task} & \textbf{Outcome} \\
\hline
Initialize repository & Repository created successfully (Figure 2-3) \\
Push to GitHub & Changes pushed and visible online (Figures 4–8) \\
Clone and Pull & Repository cloned and synced correctly (Figures 9–11) \\
GitHub Actions (Pylint) & Failed first due to docstring issues, later passed with green tick (Figures 12–21) \\
\hline
\end{tabular}
\end{center}




\section{Discussion and Conclusion}

This lab gave me hands-on practice with Git, starting from creating an empty repository to adding files, staging, committing, pushing to GitHub, cloning repositories, and pulling updates. One of the main challenges I faced was the branch mismatch issue (main vs. master), which caused errors during my first push.

I also found interpreting the Pylint workflow's yaml file slightly tricky at first and even though the Python code was running fine, the workflow still failed due to docstring and formatting issues which were caught by Pylint. Going through the error messages and fixing them was a valuable learning experience. The most satisfying moment was finally seeing the green tick on GitHub after the corrected code passed Pylint.

Overall, this lab gave me a nice overview of how Git and GitHub work together and showed me how CI/CD pipelines can give instant feedback on code quality, something that is very useful in real-world projects where many developers work together.




\section{References}
\begin{itemize}[itemsep=0.01em, topsep=0pt]
    \item \href{https://git-scm.com/doc}{Git Documentation}
    \item \href{https://docs.github.com/en}{GitHub Guides}
    \item \href{https://education.github.com/git-cheat-sheet-education.pdf}{Git Cheat Sheet}
    \item \href{https://docs.pylint.org/}{Pylint Documentation}
    \item Lab Document (Shared on Google Classroom)
\end{itemize}





\chapter{Lab 2: Mining Bug-Fixing Commits, LLM Inference, Rectifier, and Evaluation}

\section{Introduction, Setup, and Tools}

\subsection{Introduction}
This lab focused on mining open-source repositories to study bug-fixing commits and commit message alignment. I implemented a pipeline to:
\begin{enumerate}[itemsep=0.05em, topsep=0pt]
    \item Identify bug-fixing commits from a real-world project.
    \item Extract file-level diffs from those commits.
    \item Use a pre-trained LLM to generate concise summaries for each file-level change.
    \item Rectify those messages to make them more precise and context-aware.
    \item Evaluate the quality of developer, LLM, and rectified messages using semantic similarity with CodeBERT.
\end{enumerate}

The motivation behind this pipeline was that commit messages are not always reliable indicators of what a change actually fixes. Developers may batch multiple fixes, write vague messages, or skip details. Automated tools and rectifiers can help make these messages more consistent and useful.

\subsection{Environment and Tools}
\begin{itemize}[itemsep=0.05em, topsep=0pt]
    \item \textbf{Operating System:} Windows 11
    \item \textbf{Terminal:} Powershell 7
    \item \textbf{Python:} 3.13.7
    \item \textbf{PyTorch:} 2.8 (with CUDA 12.9)
    \item \textbf{Transformers:} 4.56
    \item \textbf{PyDriller:} 2.8
    \item \textbf{Models used:}
    \begin{itemize}[itemsep=0.05em, topsep=0pt]
        \item \href{https://huggingface.co/mamiksik/CommitPredictorT5}{mamiksik/CommitPredictorT5} (LLM Inference)
        \item \href{https://ollama.com/library/codellama}{codellama:7b} via Ollama (Rectifier)
        \item \href{https://huggingface.co/microsoft/codebert-base}{microsoft/codebert-base} (Evaluation)
    \end{itemize}
    \item \textbf{Repository analyzed:} \href{https://github.com/3b1b/manim}{3b1b/manim}
\end{itemize}

\insertimage[0.9\textwidth]{../images/lab2/1.png}[Environment Details]

\section{Methodology and Execution}

\subsection{Repository Selection}
For this lab, I chose the repository \href{https://github.com/3b1b/manim}{3b1b/manim}. Manim (short for Mathematical Animation Engine) is a Python library that started with Grant Sanderson’s 3Blue1Brown channel and has since grown into a large open-source project. It is mainly used to create mathematical animations and visualizations, and because of its popularity it now has a wide contributor base and frequent updates.

I felt Manim was a good choice because it’s not just an academic toy project but a tool actually used by educators, researchers, and content creators. That also means its commit history has plenty of real bug fixes to study, which fits well with the aim of this assignment.

\subsubsection{Selection Criteria}
\insertimage[0.9\textwidth]{../images/lab2/2.png}[Repository Statistics]

While narrowing down the repository, I kept the following points in mind:
\begin{enumerate}
    \item \textbf{Number of commits:} Manim has more than \textbf{6300 commits}, which is large enough to give me enough bug-fixing commits for analysis.
    \item \textbf{Popularity:} With around \textbf{78k stars} and \textbf{6.7k forks}, the project has a huge user base and community involvement, so the data is representative of real usage.
    \item \textbf{Programming language:} The project is written in \textbf{Python}, which works well since the lab tools like PyDriller and radon are also Python-based.
    \item \textbf{Relevance:} Since Manim deals with mathematical visualization and graphics, correctness and stability are very important. That makes bug-fixing commits here especially meaningful to analyze.
    \item \textbf{Active development:} The repo is still very active, with the last commit in June 2025 and contributions from over \textbf{160 developers}, showing that the project is maintained and evolving.
\end{enumerate}

Based on these reasons, Manim seemed like a balanced and practical choice for carrying out this lab. Then I cloned the repository locally using the \command{git clone <URL>} command.
\insertimage[0.8\textwidth]{../images/lab2/3.png}[Git Clone]

\subsection{Identifying Bug-Fixing Commits}
\textbf{Notebook Link:} \href{https://github.com/ShardulJunagade/cs202-stt/lab2/bugfix_commits.ipynb}{bugfix\_commits.ipynb}

I first defined a heuristic to detect bug-fixing commits. I scanned commit messages for keywords such as:
\begin{quote}
fix, bug, patch, error, issue, defect, crash, flaw, repair, resolve, solve, fail, leak, vulnerability
\end{quote}

This simple keyword filter is fast and transparent. The downside is that it may miss commits where developers did not explicitly mention a bug (false negatives), or capture irrelevant commits where the keyword appeared casually (false positives).

Using PyDriller, I traversed the commit history of the \textbf{manim} repository and stored each matching commit in a CSV (\texttt{bugfix\_commits.csv}) with the following fields:
\begin{itemize}[itemsep=0.05em, topsep=0pt]
    \item Commit hash
    \item Commit message
    \item Parent hashes
    \item Is merge commit?
    \item Modified files
\end{itemize}

The following code snippet shows the implementation:
\insertimage[0.9\textwidth]{../images/lab2/4.png}[Code for extracting Bug Fixing Commits]

I traversed 6,344 commits, out of which, the keyword filter flagged 1358 as bug-fix candidates (21\%).
\insertimage[0.9\textwidth]{../images/lab2/5.png}[Bug Fixing Commits]

\subsection{Extracting File-level Diffs}
\textbf{Notebook Link:} \href{https://github.com/ShardulJunagade/cs202-stt/lab2/diff_extract_and_llm_infer.ipynb}{diff\_extract\_and\_llm\_infer.ipynb}

Since commits often modify multiple files, I processed each file separately. For each bug-fixing commit, I extracted the before and after source codes for each modified file and also stored metadata such as filename, change type, and the git diff.

The following image shows the code implementation:
\insertimage[0.9\textwidth]{../images/lab2/6.png}[Code for extracting Per-File Diffs]

After running the code, I extracted 2041 file-level diffs and saved these entries to \texttt{diffs\_per\_file.csv}.
\insertimage[0.9\textwidth]{../images/lab2/8.png}[File-level Diffs]

\subsection{LLM Inference of ``fix type''}
\textbf{Notebook Link:} \href{https://github.com/ShardulJunagade/cs202-stt/lab2/diff_extract_and_llm_infer.ipynb}{diff\_extract\_and\_llm\_infer.ipynb}

I used the Hugging Face model \textbf{CommitPredictorT5} to infer the type of fix from the diff. I gave the following prompt template to the pretrained model.
\begin{verbatim}
File: <filename>
Diff: <diff>
\end{verbatim}
The model generated concise commit-style summaries (max length = 64 tokens). These were appended to the CSV as an extra column: \textit{LLM Inference (fix type)} and I saved this dataset to \texttt{diffs\_per\_file\_with\_llm\_infer.csv}. This allowed direct comparison between the developer written commit messages and the LLM predictions.

The following code snippet shows the implementation:
\insertimage[0.9\textwidth]{../images/lab2/9.png}[Code for LLM Inference]
\insertimage[0.9\textwidth]{../images/lab2/10.png}[LLM Inference Samples]

\subsection{Rectifier Formulation}
\textbf{Notebook Link:} \href{https://github.com/ShardulJunagade/cs202-stt/lab2/ollama_rectifier.ipynb}{ollama\_rectifier.ipynb}

Developer messages and LLM outputs can still be vague or misaligned, especially when multiple files are involved. Many times developers may not clearly specify the bug or issue being addressed. Developers often combine multiple changes/fixes in a single commit and the LLM may not capture all relevant context. To improve clarity, I designed a rectifier with the following rules:
\begin{itemize}[itemsep=0.05em, topsep=0pt]
    \item Input: file name, change type, diff, and optionally developer + LLM messages.
    \item Output style: \texttt{[file]: Fix <bug/issue> in <component> by <specific action>}
    \item Keep messages short ($<$ 20 words) and avoid vague verbs.
\end{itemize}

I implemented this rectifier with \textbf{Ollama + codellama:7b} for local inference on my NVIDIA RTX 4060 machine and added 1 more column named \textit{Rectified Message} to the CSV for the rectified messages. I saved the results to \texttt{ollama\_rectified\_commits.csv}.

The following image shows the code implementation for the rectifier:
\insertimage[0.9\textwidth]{../images/lab2/11.png}[Code for Rectifier]
\insertimage[0.9\textwidth]{../images/lab2/12.png}[Rectifier Sample Results]



\subsection{Evaluation with CodeBERT}
\textbf{Notebook Link:} \href{https://github.com/ShardulJunagade/cs202-stt/lab2/ollama_evaluation.ipynb}{ollama\_evaluation.ipynb}

To measure how well each message aligned with its code change, I used \textbf{microsoft/codebert-base}. First, I generated embeddings for both the diff text and the corresponding messages (developer, LLM, and rectified). Then, I computed the cosine similarity between these embeddings to quantify how semantically close each message was to the actual code change. In this setup, a higher similarity score indicates a stronger alignment between the text and the code.

I decided to keep the threshold for precision at 0.9. Any score greater than 0.9 was considered “precise.” Using this rule, I could compute the hit rate for each category of message and answer the three research questions (RQ1–RQ3). The results were saved to \texttt{ollama\_scores\_codebert.csv}. The following image shows the code implementation for the evaluation:

\insertimage[0.8\textwidth]{../images/lab2/13.png}[Code for Evaluation]
\insertimage[0.8\textwidth]{../images/lab2/14.png}[CodeBERT Scores]
\insertimage[0.9\textwidth]{../images/lab2/15.png}[Mean Hit Rates]

We can see that the developer messages had a precision hit rate of 91\%, the LLM inference messages had a hit rate of 92\%, and the rectified messages improved the hit rate to 100\%. This indicates that the rectifier was effective in enhancing the alignment between the messages and the code changes.

\section{Results and Analysis}

\textbf{Bug-fixing commits found:} 1358\\
\textbf{File-level diffs extracted:} 2041

Following are few examples of developer messages, LLM inferences, and my rectified messages:

\begin{tabular}{|p{4cm}|p{4cm}|p{7cm}|}
\hline
\textbf{Developer Message} & \textbf{LLM Inference} & \textbf{Rectified Message} \\
\hline
A few fixes to initial point\_thickness implementation & add missing constants & [constants.py]: Fix default point thickness to be 4 in DEFAULT\_POINT\_THICKNESS by specifying it explicitly. \\
\hline
A few fixes to initial point\_thickness implementation & add nudge to displayer.py & [displayer.py]: Fix potential offset issue in point\_thickness by adjusting the thickness of a plus-sign-shaped pixel arrangement to ensure correct rendering on high-quality displays. \\
\hline
A few fixes to initial point\_thickness implementation & add missing docstring & [mobject.py]: Fix inconsistent point\_thickness implementation in Mobject1D and Mobject2D by specifying a default value for the attribute. \\
\hline
\end{tabular}

\vspace{1em}

\begin{tabular}{|c|l|c|}
\hline
\textbf{RQ}  & \textbf{Message Type}    & \textbf{Hit Rate (threshold = 0.9)} \\
\hline
RQ1 & Developer messages  & $\sim$91\% \\
RQ2 & LLM inference       & $\sim$93\% \\
RQ3 & Rectified messages  & 100\% \\
\hline
\end{tabular}

Developer messages were often short and lacked detail, lowering their alignment scores. The LLM inference was generally good but sometimes missed context that the rectifier captured.

My rectifier is able to consistently produce high-quality, precise messages. This is mainly because of 3 reasons:
\begin{enumerate}
    \item \textbf{Better Large Language Model (LLM):} The use of a more advanced LLM for inference likely contributed to the improved message quality. The LLM was able to better understand the context and nuances of the code changes, resulting in more accurate and relevant messages.
    \item \textbf{Better Context:} By using the specific file name and change type as part of the input, the rectifier can generate messages that are more closely aligned with the actual code changes being made. This helps to reduce ambiguity and improve precision.
    \item \textbf{Structured Output:} The output format of the rectifier is designed to be concise and specific, which helps to ensure that the messages are clear and actionable.
\end{enumerate}






\section{Discussion and Conclusion}

\subsection{Challenges faced}
During the lab, I faced a few challenges that slowed me down initially. PyDriller, for example, was a new library for me, and I needed some time to get comfortable with its API and how to extract the right commit-level information. Another difficulty came from the keyword-based heuristic I used to identify bug-fixing commits. While it worked reasonably well, it sometimes missed commits where developers didn’t explicitly mention bug-related terms, and on the other hand it also pulled in a few extra commits that weren’t true bug fixes. Finally, token limits posed a practical issue -- some of the larger diffs had to be truncated before being passed to the model, and this occasionally hurt the accuracy of the LLM’s generated summaries.

\subsection{Lessons learned}
Working through these issues taught me a few important lessons. I found that analyzing changes at the file level and then applying rectification added real value, because it made commit messages more precise and easier to interpret. I also realized that building a pipeline by combining several tools -- PyDriller for mining, Hugging Face models for inference, Ollama for rectification, and CodeBERT for evaluation -- can be powerful, but it also demands a lot of care in data handling. Even small oversights, like inconsistent CSV column names, can break later steps in the workflow.

\subsection{Conclusion}
Overall, the end-to-end pipeline -- from mining commits to generating diffs, running LLM inference, rectifying the outputs, and finally evaluating them -- proved to be quite workable. The rectifier in particular helped improve the quality of commit messages, often making them more specific and useful than both the original developer-written messages and the raw LLM predictions. Although the approach is not perfect, it serves as a strong starting point for building datasets and improving maintainability in open-source projects.

\section{References}
\begin{itemize}[itemsep=0.05em, topsep=0pt]
    \item \href{https://pydriller.readthedocs.io}{PyDriller}
    \item \href{https://huggingface.co/docs/transformers}{Hugging Face Transformers}
    \item \href{https://huggingface.co/microsoft/codebert-base}{CodeBERT (microsoft/codebert-base)}
    \item \href{https://huggingface.co/mamiksik/CommitPredictorT5}{CommitPredictorT5 (mamiksik/CommitPredictorT5)}
    \item \href{https://ollama.ai}{Ollama}
    \item \href{https://github.com/3b1b/manim}{Repository analyzed (manim)}
    \item Lab Document: \href{https://drive.google.com/file/d/1L4pCGQCekeELjgOJ4HxQ-Tso0QaOKIDf/view}{Google Doc}
\end{itemize}








\end{document}
